{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import hddm\n",
    "import os\n",
    "from datetime import datetime\n",
    "from timeit import default_timer as timer\n",
    "from datetime import timedelta\n",
    "from scipy.stats import spearmanr\n",
    "import statistics\n",
    "from zepid.graphics import EffectMeasurePlot\n",
    "import pingouin as pg\n",
    "from pymer4.models import Lmer\n",
    "\n",
    "plt.close('all')\n",
    "beh_path = ('/Users/nagrodzkij/data/angry/')\n",
    "input_path = ('/Users/nagrodzkij/data/angry/input/')\n",
    "output_path = ('/Users/nagrodzkij/data/angry/output/')\n",
    "\n",
    "FaceScores = pd.read_csv(input_path+'/demog/FaceScores.csv',index_col=[0])\n",
    "\n",
    "prev_depressed = pd.read_csv(input_path+'/demog/previously_depressed.csv', names=['0'])\n",
    "list_depressed = prev_depressed['0'].to_list()\n",
    "\n",
    "demog = pd.read_csv(output_path+'table_demog_byccid.csv')\n",
    "\n",
    "################################\n",
    "\n",
    "os.chdir(output_path+'hddm/')\n",
    "hddm_path=(output_path+'hddm/')\n",
    "accuracy_coding_path = hddm_path+'accuracy_coding/'\n",
    "accuracy_coding_models_path=accuracy_coding_path+'models/'\n",
    "\n",
    "data = hddm.load_csv(output_path+'data_emoFace_excl.csv')\n",
    "demog = pd.read_csv(output_path+'table_demog_withedu_byccid.csv',index_col=[0])\n",
    "age = demog.age\n",
    "FaceScores = pd.read_csv(input_path+'/demog/FaceScores.csv',index_col=[0])\n",
    "\n",
    "# Functions for correlations\n",
    "\n",
    "def mode(arr):\n",
    "    if len(arr) == 0:\n",
    "        return []\n",
    "\n",
    "    frequencies = {}\n",
    "\n",
    "    for num in arr:\n",
    "        frequencies[num] = frequencies.get(num,0) + 1\n",
    "\n",
    "    mode = max([value for value in frequencies.values()])\n",
    "\n",
    "    modes = []\n",
    "\n",
    "    for key in frequencies.keys():\n",
    "        if frequencies[key] == mode:\n",
    "            modes.append(key)\n",
    "\n",
    "    return modes\n",
    "\n",
    "def bayesian_probability(posterior_distribution):\n",
    "    ''' Calculates probability that a posterior distribution is different from 0\n",
    "    Parameters\n",
    "    --------------------\n",
    "    posterior_distribution : iterable object such as a list or np.array\n",
    "\n",
    "    Returns\n",
    "    --------------------\n",
    "    perc : a float (in percent), percentage probability that the posterior distribution is different from 0\n",
    "    '''\n",
    "    j = posterior_distribution\n",
    "    if statistics.mean(j) > 0:\n",
    "        corr_values = [i for i in j if i > 0]\n",
    "        perc = len(corr_values) / len(j) * 100\n",
    "    else:\n",
    "        corr_values = [i for i in j if i < 0]\n",
    "        perc = len(corr_values) / len(j) * 100\n",
    "\n",
    "    return perc\n",
    "\n",
    "def pearsonr_ci(x,y,alpha=0.05):\n",
    "    ''' calculate Pearson correlation along with the confidence interval using scipy and numpy\n",
    "    Parameters\n",
    "    ----------\n",
    "    x, y : iterable object such as a list or np.array\n",
    "      Input for correlation calculation\n",
    "    alpha : float\n",
    "      Significance level. 0.05 by default\n",
    "    Returns\n",
    "    -------\n",
    "    r : float\n",
    "      Pearson's correlation coefficient\n",
    "    pval : float\n",
    "      The corresponding p value\n",
    "    lo, hi : float\n",
    "      The lower and upper bound of confidence intervals\n",
    "    '''\n",
    "\n",
    "    r, p = statistics.pearsonr(x,y)\n",
    "    r_z = np.arctanh(r)\n",
    "    se = 1/np.sqrt(x.size-3)\n",
    "    z = statistics.norm.ppf(1-alpha/2)\n",
    "    lo_z, hi_z = r_z-z*se, r_z+z*se\n",
    "    lo, hi = np.tanh((lo_z, hi_z))\n",
    "    return r, p, lo, hi\n",
    "\n",
    "def ddm_param_corr(ddm_param,covariate,ddm,cov,save):\n",
    "    ''' calculate Spearmann's rank correlation coefficient between DDM parameter and any covariate (must be 1 value per participant)\n",
    "    Parameters\n",
    "    ----------\n",
    "    ddm_param : dataframe with any number of values of DDM parameters per participant\n",
    "    covariate : iterable object such as a list or np.array\n",
    "      Input for correlation calculation\n",
    "    ddm : name of ddm_param being correlated\n",
    "    cov : name of covariate as will be visible in the saved file (e.g. when name = 'age', saved file is rho_vsens_age.csv)\n",
    "    save : whether you want to save the output; =1 will save the output in a csv file\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "\n",
    "    dt : dictionary of values of rho, p-values, and significance levels\n",
    "    df : dataframe of values of rho, p-vales, and significance levels\n",
    "    '''\n",
    "\n",
    "    dt = {'rho': [], 'p-value': [], 'sig': []}\n",
    "\n",
    "    if len(ddm_param.columns[0][:6]) > 1:\n",
    "        ddm_param_ind = ddm_param.transpose()\n",
    "        check = 1\n",
    "    elif len(ddm_param.index[0][:6]) > 1:\n",
    "        ddm_param_ind = ddm_param\n",
    "        check = 1\n",
    "    else:\n",
    "        check = 0\n",
    "\n",
    "    assert check == 1, 'Incorrect input type' # Checks if v_sens is of the correct input type\n",
    "\n",
    "    for i in range(0, len(ddm_param_ind.columns)):\n",
    "        ddm_param_run = ddm_param_ind.iloc[:, [i]]\n",
    "        data1 = ddm_param_run\n",
    "        data2 = covariate\n",
    "\n",
    "        coef, p = spearmanr(data1, data2)\n",
    "        dt['rho'].append(coef)\n",
    "        dt['p-value'].append(p)\n",
    "\n",
    "        if p > 0.05:\n",
    "            dt['sig'].append('ns')\n",
    "        elif 0.01 < p <= 0.05:\n",
    "            dt['sig'].append('*')\n",
    "        elif 0.001 < p <= 0.01:\n",
    "            dt['sig'].append('**')\n",
    "        elif p <= 0.001:\n",
    "            dt['sig'].append('***')\n",
    "\n",
    "    df = pd.DataFrame.from_dict(dt)\n",
    "    if save == 1:\n",
    "        df.to_csv('/Users/nagrodzkij/data/driftmodel/emo_recog_hddm/rho_%s_%s.csv' %(ddm,cov))\n",
    "\n",
    "    return dt, df\n",
    "\n",
    "def trim_to_hdi(df,perc):\n",
    "    ''' Trim a dataframe by a specified amount on each side to obtain a Bayesian high density interval from rho values\n",
    "    Parameters\n",
    "    ----------\n",
    "    df : dataframe containing column 'rho' with values of rho\n",
    "    perc : percentage specifying the extent of the high-density interval, e.g. for a 95% HDI specify 95\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    df_trimmed : dataframe containing trimmed values\n",
    "    '''\n",
    "\n",
    "    assert perc <=100, 'Specify extent of HDI as a percentage e.g. 95 for a 95% HDI'\n",
    "    assert perc > 70, 'Specify extent of HDI as a percentage e.g. 95 for a 95% HDI'\n",
    "    each_side = (100 - perc)/2\n",
    "    df_sorted = np.array(sorted(df['rho']))\n",
    "    trim = int(each_side*df_sorted.size/100)\n",
    "    df_trimmed = df_sorted[trim:-trim]\n",
    "\n",
    "    return df_trimmed\n",
    "\n",
    "def add_values_in_dict(dict, key, list_of_values):\n",
    "    ''' Append multiple values to a key in\n",
    "        the given dictionary '''\n",
    "    if key not in dict:\n",
    "        dict[key] = list_of_values\n",
    "    else:\n",
    "        dict[key].extend(list_of_values)\n",
    "    return dict\n",
    "\n",
    "# Functions for fitting DDM\n",
    "\n",
    "def get_combined_traces(models):\n",
    "    traces = []\n",
    "    for i in range(len(models)):\n",
    "        m = models[i]\n",
    "        trace = m.get_traces()\n",
    "        trace['chain'] = i + 1\n",
    "        traces.append(trace)\n",
    "\n",
    "    return traces\n",
    "\n",
    "def run_hddm_stimcoding(id, data2fit, dependency, folder, name, incl, split_by,group_nodes):\n",
    "    print()\n",
    "    print('running model %i' % id)\n",
    "\n",
    "    if split_by == ():\n",
    "        dbname = '%s/models/%s_%i.db' % (folder, name, id)\n",
    "        mname = '%s/models/%s_%i' % (folder, name, id)\n",
    "        m = hddm.HDDMStimCoding(data2fit,\n",
    "                                p_outlier=0.05,\n",
    "                                include=incl,\n",
    "                                stim_col='stim',\n",
    "                                depends_on=dependency,\n",
    "                                group_only_nodes='%s' % group_nodes\n",
    "                                )\n",
    "\n",
    "        m.find_starting_values()\n",
    "        m.sample(10000, burn=3000, thin=5, dbname=dbname, db='pickle')\n",
    "\n",
    "        m.save(mname)\n",
    "\n",
    "        return m\n",
    "\n",
    "    elif split_by == 'v':\n",
    "        dbname = '%s/models/%s_%i.db' % (folder, name, id)\n",
    "        mname = '%s/models/%s_%i' % (folder, name, id)\n",
    "        m = hddm.HDDMStimCoding(data2fit,\n",
    "                                p_outlier=0.05,\n",
    "                                include=incl,\n",
    "                                split_param='v',\n",
    "                                stim_col='stim',\n",
    "                                depends_on=dependency,\n",
    "                                group_only_nodes='%s' % group_nodes\n",
    "                                )\n",
    "\n",
    "        m.find_starting_values()\n",
    "        m.sample(10000, burn=3000, thin=5, dbname=dbname, db='pickle')\n",
    "\n",
    "        m.save(mname)\n",
    "\n",
    "        return m\n",
    "\n",
    "    elif split_by == 'z':\n",
    "        dbname = '%s/models/%s_%i.db' % (folder, name, id)\n",
    "        mname = '%s/models/%s_%i' % (folder, name, id)\n",
    "        m = hddm.HDDMStimCoding(data2fit,\n",
    "                                p_outlier=0.05,\n",
    "                                include=incl,\n",
    "                                split_param='z',\n",
    "                                stim_col='stim',\n",
    "                                depends_on=dependency,\n",
    "                                )\n",
    "\n",
    "        m.find_starting_values()\n",
    "        m.sample(10000, burn=3000, thin=5, dbname=dbname, db='pickle')\n",
    "        m.save(mname)\n",
    "\n",
    "        return m\n",
    "\n",
    "def run_hddm_stimcoding_multiple(data2fit, dependency, folder, name, incl, split_by, num_chains,group_nodes):\n",
    "    for i in np.arange(3, num_chains+1):\n",
    "        id = i\n",
    "        run_hddm_stimcoding(id, data2fit, dependency, folder, name, incl, split_by,group_nodes)\n",
    "\n",
    "    return\n",
    "\n",
    "def hddm_save_trace(name, save=1):\n",
    "    models = []\n",
    "    for ident in np.arange(1, 5):\n",
    "        mname = accuracy_coding_path+'models/'+name+'_'+str(ident)\n",
    "        m_stim = hddm.load(mname)\n",
    "        models.append(m_stim)\n",
    "\n",
    "        params = ('a', 't', 'v')\n",
    "        for param in params:\n",
    "            plot = m_stim.plot_posteriors(param)\n",
    "            plot = plt.gcf()\n",
    "            plot.savefig(accuracy_coding_models_path+name+'_'+str(ident)+param+'.jpg')\n",
    "        plt.close('all')\n",
    "\n",
    "    # rhat = hddm.analyze.gelman_rubin(models)  # get the R hat information\n",
    "\n",
    "    if save:\n",
    "        traces = get_combined_traces(models)\n",
    "        traces = pd.concat(traces)\n",
    "        traces.to_csv(accuracy_coding_models_path+name+'_trace.csv')\n",
    "\n",
    "    # comb = kabuki.utils.concat_models(models)\n",
    "    return models\n",
    "\n",
    "def plot_dep_vcond(name):\n",
    "    m = hddm.load(name)\n",
    "    v_AngryMale, v_AngryFemale, v_NeutralMale, v_NeutralFemale = \\\n",
    "        m.nodes_db.node[['v(Angry.0)', 'v(Angry.1)', 'v(Neutral.0)', 'v(Neutral.1)']]\n",
    "    hddm.analyze.plot_posterior_nodes([v_AngryMale, v_AngryFemale, v_NeutralMale, v_NeutralFemale])\n",
    "    plt.xlabel('Drift rate')\n",
    "    plt.ylabel('Posterior probability')\n",
    "    plt.title(f'Posterior of drift-rate group means. Model ' + name)\n",
    "    plot = plt.gcf()\n",
    "    plot.savefig(name + '_plot.jpg')\n",
    "    plt.show()\n",
    "\n",
    "    return plot, m\n",
    "\n",
    "def plot_dics(name):\n",
    "    dics = hddm.load_csv('%s_dics.csv' % (name))\n",
    "\n",
    "    # This ensures the figure size matches the number of models being plotted\n",
    "    n = len(dics) + 1\n",
    "    pix = 0.3 * n\n",
    "\n",
    "    dics.plot(x='id', y='dic', marker='o', legend=False)\n",
    "    fig = plt.gcf()\n",
    "    fig.set_size_inches(pix, 5)\n",
    "\n",
    "    # Setting axes and title\n",
    "    plt.xlim([0, n])\n",
    "    plt.xticks(np.arange(0, n, 1.0))\n",
    "    plt.title(\n",
    "        'Graph of DIC by model number\\n'\n",
    "        # '(best model is marked in red, the second and third best models are marked in black)'\n",
    "        )\n",
    "    plt.xlabel('Model ID')\n",
    "    plt.ylabel('DIC')\n",
    "\n",
    "    # Make a grid\n",
    "    minor_ticks = np.arange(1.5,29,1)\n",
    "    major_ticks = np.arange(0,31,1)\n",
    "\n",
    "    ax = plt.gca()\n",
    "    ax.set_xticks(minor_ticks, minor=True)\n",
    "    ax.set_xticks(major_ticks)\n",
    "\n",
    "    ax.grid(which='minor',alpha=0.6)\n",
    "\n",
    "    # Marking the top 3 models\n",
    "    dics_sorted = dics.sort_values('dic', ascending=True)\n",
    "    plt.ylim([int(dics_sorted.iloc[0]['dic']) * 0.7, int(dics_sorted.iloc[len(dics) - 1]['dic']) * 1.1])\n",
    "\n",
    "    for iBest in np.arange(0, 3):\n",
    "        best_iBest = int(dics_sorted.iloc[iBest]['dic'])\n",
    "        bestmodel_iBest = int(dics_sorted.iloc[iBest]['id'])\n",
    "        fig = plt.gcf()\n",
    "        if iBest == 0:\n",
    "            plt.plot(bestmodel_iBest, best_iBest, 'ro')\n",
    "            plt.annotate(\"%i\" % (bestmodel_iBest), (bestmodel_iBest, best_iBest * 0.8), ha='center')\n",
    "        else:\n",
    "            plt.plot(bestmodel_iBest, best_iBest, 'ko')\n",
    "            plt.annotate(\"%i\" % (bestmodel_iBest), (bestmodel_iBest, best_iBest * 0.8), ha='center')\n",
    "\n",
    "    # Save\n",
    "    fig = plt.gcf()\n",
    "    fig.savefig('testing_all_dic_plot.jpg')\n",
    "    plt.show()\n",
    "\n",
    "    return n, pix\n",
    "\n",
    "def models_stimcoding_from_matrix(data2fit, filename, folder, name, group_nodes):\n",
    "    \"\"\"This function runs multiple models specified in a csv file.\n",
    "\n",
    "        Arguments\n",
    "        - data2fit: data frame with data\n",
    "        - filename: name of csv file with matrix\n",
    "        - name: the required name of the run of models e.g. 'test' or 'final'\n",
    "        - folder: path to the directory which has the csv file and a folder models in which the models will be saved\n",
    "\n",
    "        Requirements\n",
    "        - csv file needs to be created from MASTER_model_matrix (transposed) or from scratch\n",
    "        - it needs to have a column with model numbers without a heading followed by columns named:\n",
    "                            \"include = 'z' \", \"depends_on = {'v':'cond'}\",\n",
    "                            \"depends_on = {'v':'stim'}\", \"depends_on = {'z':'cond'}\",\n",
    "                            \"depends_on = {'z':'stim'}\", \"depends_on = {'a':'cond'}\",\n",
    "                            \"depends_on = {'a':'stim'}\", \"split_param = 'v'\",\n",
    "                            \"split_param = 'z'\"\n",
    "        - to indicate a parameter is used, cell value = 1 and to indicate it isn't used leave blank or = 0\n",
    "\n",
    "        Saves\n",
    "        - a txt file named report_name.txt which describes the characteristics of each model and DIC\n",
    "        - models named name_1, name_2, ...\n",
    "        - a csv file of all DICs by model ID\n",
    "        \"\"\"\n",
    "\n",
    "    # Read file and rename column names to something that will work with the code\n",
    "    df = pd.read_csv(filename)\n",
    "    df.rename(columns={\"Unnamed: 0\": \"num\", \"include = 'z' \": \"z\", \"depends_on = {'v':'cond'}\": \"v : cond\",\n",
    "                       \"depends_on = {'v':'stim'}\": \"v : stim\", \"depends_on = {'z':'cond'}\": \"z : cond\",\n",
    "                       \"depends_on = {'z':'stim'}\": \"z : stim\", \"depends_on = {'a':'cond'}\": \"a : cond\",\n",
    "                       \"depends_on = {'a':'stim'}\": \"a : stim\", \"split_param = 'v'\": \"split_param = v\",\n",
    "                       \"split_param = 'z'\": \"split_param = z\"}, inplace=True)\n",
    "    df.replace(np.nan, 0, inplace=True)  # Replace all NaN (empty cells) with zeros\n",
    "\n",
    "    f = open('%s/models/report_%s_dic.txt' % (folder, name), \"w+\")\n",
    "    f.write(\"Log of drift diffusion model fitting\\n\")\n",
    "    start = timer()\n",
    "    f.write((\"Script commenced at \" + datetime.now().strftime(\"%d/%m/%Y %H:%M:%S\") + \"\\n\\n\\n\"))\n",
    "    f.write((\"Let's go!\\n\\n\\n\"))\n",
    "    f.close()\n",
    "\n",
    "    rows = len(df.index)\n",
    "    dics = {'id': [], 'dic': []}\n",
    "    for iMat in np.arange(0, rows):\n",
    "        err = 0\n",
    "        id = iMat + 1\n",
    "\n",
    "        # Check if z is included\n",
    "        if df.iloc[iMat]['z'] == 1:\n",
    "            incl = 'z'\n",
    "        else:\n",
    "            incl = ()\n",
    "\n",
    "        # Obtain split_param\n",
    "        split_by = ()\n",
    "        check_split = df.iloc[iMat, 8:10] == 1\n",
    "        k_true = list(check_split.loc[check_split == True].index)\n",
    "        if len(k_true) > 1:\n",
    "            err = 1\n",
    "            continue\n",
    "        elif len(k_true) == 1:\n",
    "            split_by = '%s' % (k_true[0][-1])\n",
    "        elif len(k_true) == 0:\n",
    "            split_by = ()\n",
    "\n",
    "        # Obtain dependency\n",
    "        dependency = {}\n",
    "        check_dep = df.iloc[iMat, 2:8] == 1\n",
    "        check_true = check_dep.loc[check_dep == True]\n",
    "        l_true = list(check_true.index)\n",
    "        params = list()\n",
    "\n",
    "        for iL in np.arange(0, len(l_true)):\n",
    "            param = l_true[iL][0]\n",
    "            if param in params:\n",
    "                dependency['%s' % (param)] = ['cond', 'stim']\n",
    "            else:\n",
    "                params.append(param)\n",
    "                dependency['%s' % (param)] = '%s' % (l_true[iL][4:])\n",
    "\n",
    "        # Run the model\n",
    "        m = run_hddm_stimcoding(id, data2fit, dependency, folder, name, incl, split_by, group_nodes)\n",
    "\n",
    "        # os.chdir('%s/models' % (folder))\n",
    "        # m = hddm.load('%s_%i' % (name, id))\n",
    "\n",
    "        dic = m.dic\n",
    "        dics['id'].append('%i' % (id))\n",
    "        dics['dic'].append('%f' % (dic))\n",
    "\n",
    "        # Prints characteristics of each model in the log file\n",
    "        f = open('%s/models/report_%s_dic.txt' % (folder, name), \"a\")\n",
    "        f.write((f'Model ' + str(id) + \"\\n\"))\n",
    "        f.write(('    include ' + str(incl) + \"\\n\"))\n",
    "        f.write(('    split_by ' + str(split_by) + \"\\n\"))\n",
    "        f.write(('    dependency ' + str(dependency) + \"\\n\"))\n",
    "        f.write(('    DIC: ' + str(dic) + \"\\n\"))\n",
    "        if err == 1:\n",
    "            f.write('Something went wrong! Splitting by more than 1 parameter' + \"\\n\")\n",
    "        f.write(\"\\n\")\n",
    "        f.close()\n",
    "\n",
    "    f = open('%s/models/report_%s_dic.txt' % (folder, name), \"a\")\n",
    "    f.write((\"DDM fitting complete!\\n\"))\n",
    "    min_dic = min(dics, key=lambda x: x[1])[1]\n",
    "    model_min_dic = min(dics, key=lambda x: x[1])[0]\n",
    "    f.write((\"The best fitting model is model \" + str(model_min_dic) + \" with a DIC of \" + str(min_dic) + \"\\n\\n\\n\"))\n",
    "\n",
    "    dics_df = pd.DataFrame(dics)\n",
    "    dics_df.to_csv('%s_dics.csv' % (name), index=False)\n",
    "\n",
    "    end = timer()\n",
    "    f.write((\"Script finished at \" + datetime.now().strftime(\"%d/%m/%Y %H:%M:%S\") + \"\\n\"))\n",
    "    f.write((\"Total runtime: \" + str(timedelta(seconds=end - start))))\n",
    "    f.close()\n",
    "\n",
    "    return rows, f, dics\n",
    "\n",
    "def run_hddm(ident, data2fit, dependency, folder, name, sample_no, burn_no, thin_no, participant_model):\n",
    "    print()\n",
    "    print('running model %i' % ident)\n",
    "\n",
    "\n",
    "    dbname = '%smodels/%s_%i.db' % (folder, name, ident)\n",
    "    mname = '%smodels/%s_%i' % (folder, name, ident)\n",
    "    m = hddm.HDDM(data2fit,\n",
    "                            p_outlier=0.05,\n",
    "                            depends_on=dependency,\n",
    "                            is_group_model=participant_model,\n",
    "                            )\n",
    "\n",
    "    m.find_starting_values()\n",
    "    m.sample(sample_no, burn=burn_no, thin=thin_no, dbname=dbname, db='pickle')\n",
    "\n",
    "    m.save(mname)\n",
    "\n",
    "    return m\n",
    "\n",
    "def models_from_matrix(data2fit, filename, folder, name, sample_no, burn_no, thin_no,participant_model):\n",
    "    \"\"\"This function runs multiple models specified in a csv file.\n",
    "\n",
    "        Arguments\n",
    "        - data2fit: data frame with data\n",
    "        - filename: name of csv file with matrix\n",
    "        - name: the required name of the run of models e.g. 'test' or 'final'\n",
    "        - folder: path to the directory which has the csv file and a folder models in which the models will be saved\n",
    "\n",
    "        Requirements\n",
    "        - csv file needs to be created from MASTER_model_matrix (transposed) or from scratch\n",
    "        - it needs to have a column with model numbers without a heading followed by columns named:\n",
    "                            \"depends_on = {'v':'cond'}\",\n",
    "                            \"depends_on = {'z':'cond'}\",\n",
    "                            \"depends_on = {'a':'cond'}\",\n",
    "        - to indicate a parameter is used, cell value = 1 and to indicate it isn't used leave blank or = 0\n",
    "\n",
    "        Saves\n",
    "        - a txt file named report_name.txt which describes the characteristics of each model and DIC\n",
    "        - models named name_1, name_2, ...\n",
    "        - a csv file of all DICs by model ID\n",
    "        \"\"\"\n",
    "\n",
    "    # Read file and rename column names to something that will work with the code\n",
    "    df = pd.read_csv(filename)\n",
    "    df.rename(columns={\"Unnamed: 0\": \"num\", \"depends_on = {'v':'cond'}\": \"v : cond\",\n",
    "                       \"depends_on = {'z':'cond'}\": \"z : cond\",\n",
    "                       \"depends_on = {'a':'cond'}\": \"a : cond\",\n",
    "                       \"depends_on = {'t':'cond'}\": \"t : cond\"\n",
    "                       },\n",
    "              inplace=True)\n",
    "    df.replace(np.nan, 0, inplace=True)  # Replace all NaN (empty cells) with zeros\n",
    "\n",
    "    f = open('%smodels/report_%s_dic.txt' % (folder, name), \"w+\")\n",
    "    f.write(\"Log of drift diffusion model fitting\\n\")\n",
    "    start = timer()\n",
    "    f.write(\"Script commenced at \" + datetime.now().strftime(\"%d/%m/%Y %H:%M:%S\") + \"\\n\\n\\n\")\n",
    "    f.write(\"Let's go!\\n\\n\\n\")\n",
    "    f.close()\n",
    "\n",
    "    rows = len(df.index)\n",
    "    dics = {'id': [], 'dic': []}\n",
    "    for iMat in np.arange(0, rows):\n",
    "        models=[]\n",
    "        err = 0\n",
    "        ident = iMat + 1\n",
    "\n",
    "        # Obtain dependency\n",
    "        dependency = {}\n",
    "        check_dep = df.iloc[iMat, 1:6] == 1\n",
    "        check_true = check_dep.loc[check_dep == True]\n",
    "        l_true = list(check_true.index)\n",
    "        params = list()\n",
    "\n",
    "        for iL in np.arange(0, len(l_true)):\n",
    "            param = l_true[iL][0]\n",
    "            if param in params:\n",
    "                dependency['%s' % (param)] = ['cond', 'stim']\n",
    "            else:\n",
    "                params.append(param)\n",
    "                dependency['%s' % (param)] = '%s' % (l_true[iL][4:])\n",
    "\n",
    "        # Run the model\n",
    "        m = run_hddm(ident, data2fit, dependency, folder, name, sample_no, burn_no, thin_no,participant_model)\n",
    "\n",
    "        # os.chdir('%s/models' % (folder))\n",
    "        # m = hddm.load('%s_%i' % (name, id))\n",
    "\n",
    "        dic = m.dic\n",
    "        dics['id'].append('%i' % (ident))\n",
    "        dics['dic'].append('%f' % (dic))\n",
    "\n",
    "        # Prints characteristics of each model in the log file\n",
    "        f = open('%smodels/report_%s_dic.txt' % (folder, name), \"a\")\n",
    "        f.write((f'Model ' + str(ident) + \"\\n\"))\n",
    "        f.write(('    dependency ' + str(dependency) + \"\\n\"))\n",
    "        f.write(('    DIC: ' + str(dic) + \"\\n\"))\n",
    "        if err == 1:\n",
    "            f.write('Something went wrong! Splitting by more than 1 parameter' + \"\\n\")\n",
    "        f.write(\"\\n\")\n",
    "        f.close()\n",
    "\n",
    "    f = open('%smodels/report_%s_dic.txt' % (folder, name), \"a\")\n",
    "    f.write((\"DDM fitting complete!\\n\"))\n",
    "    min_dic = min(dics, key=lambda x: x[1])[1]\n",
    "    model_min_dic = min(dics, key=lambda x: x[1])[0]\n",
    "    f.write((\"The best fitting model is model \" + str(model_min_dic) + \" with a DIC of \" + str(min_dic) + \"\\n\\n\\n\"))\n",
    "\n",
    "    dics_df = pd.DataFrame(dics)\n",
    "    dics_df.to_csv(accuracy_coding_path+'/models/'+name+'_dics.csv', index=False)\n",
    "\n",
    "    hddm_save_trace(name,save=1)\n",
    "\n",
    "    end = timer()\n",
    "    f.write((\"Script finished at \" + datetime.now().strftime(\"%d/%m/%Y %H:%M:%S\") + \"\\n\"))\n",
    "    f.write((\"Total runtime: \" + str(timedelta(seconds=end - start))))\n",
    "    f.close()\n",
    "\n",
    "    return rows, f, dics\n",
    "\n",
    "def run_hddm_multiple(data2fit, dependency, folder, name, num_chains, sample_no, burn_no, thin_no,participant_model):\n",
    "    for i in np.arange(1, num_chains+1):\n",
    "        ident = i\n",
    "        run_hddm(ident, data2fit, dependency, folder, name, sample_no, burn_no, thin_no,participant_model)\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "outputs": [],
   "source": [
    "data = data[data[\"rt\"] > 0.25]\n",
    "\n",
    "# Format the data frames correctly for HDDM modelling\n",
    "data.rename(\n",
    "    columns={\"stim_col\": \"stim\", \"condition\": \"cond\"}, inplace=True)\n",
    "\n",
    "data_hddm = data.drop(columns=['sot','duration','button','trial'])\n",
    "\n",
    "# Create data frame in which the 'response' column indicates if response is correct or incorrect\n",
    "response_acc = (data['response'] == data['stim']) + 0\n",
    "data_acc = pd.DataFrame()\n",
    "data_acc['rt'] = data['rt']\n",
    "data_acc['response'] = response_acc\n",
    "data_acc['subj_idx'] = data['subj_idx']\n",
    "data_acc['cond'] = data['cond']\n",
    "\n",
    "data_hddm.to_csv(output_path+'data_emoFace_hddm.csv')\n",
    "data_acc.to_csv(output_path+'accdata_emoFace_hddm.csv')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "running model 1\n",
      "-3489.992061015614\n",
      "-3489.98955060257\n",
      " [-----------------100%-----------------] 5000 of 5000 complete in 506.3 sec\n",
      "running model 2\n",
      "-7200.526834057502\n",
      "-7200.515628240032\n",
      " [-----------------100%-----------------] 5000 of 5000 complete in 490.6 sec\n",
      "running model 3\n",
      "-3468.7378961733643\n",
      "-3468.7376386515452\n",
      " [-----------------100%-----------------] 5000 of 5000 complete in 512.9 sec\n",
      "running model 4\n",
      "-3488.2148806672494\n",
      "-3488.2038800707187\n",
      " [-----------------100%-----------------] 5000 of 5000 complete in 558.3 secPlotting a\n",
      "Plotting t\n",
      "Plotting v\n",
      "Plotting a\n",
      "Plotting t\n",
      "Plotting v(Angry)\n",
      "Plotting v(Neutral)\n",
      "Plotting a(Angry)\n",
      "Plotting a(Neutral)\n",
      "Plotting t\n",
      "Plotting v\n",
      "Plotting a\n",
      "Plotting t(Angry)\n",
      "Plotting t(Neutral)\n",
      "Plotting v\n",
      "\n",
      "running model 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/nagrodzkij/opt/anaconda3/envs/untitled/lib/python3.6/site-packages/scipy/optimize/optimize.py:1985: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  tmp2 = (x - v) * (fx - fw)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " [-----------------100%-----------------] 10001 of 10000 complete in 6953.1 sec-----------------81%----------        ] 8130 of 10000 complete in 5693.4 sec\n",
      "running model 2\n",
      " [-----------------100%-----------------] 10001 of 10000 complete in 7035.0 sec-----------------77%---------         ] 7751 of 10000 complete in 5440.0 sec\n",
      "running model 3\n",
      " [-----------------100%-----------------] 10001 of 10000 complete in 6727.3 sec\n",
      "running model 4\n",
      " [-----------------100%-----------------] 10001 of 10000 complete in 6681.7 sec\n",
      "running model 5\n",
      " [-----------------100%-----------------] 10001 of 10000 complete in 6668.6 secPlotting a\n",
      "Plotting t\n",
      "Plotting v(Angry)\n",
      "Plotting v(Neutral)\n",
      "Plotting a\n",
      "Plotting t\n",
      "Plotting v(Angry)\n",
      "Plotting v(Neutral)\n",
      "Plotting a\n",
      "Plotting t\n",
      "Plotting v(Angry)\n",
      "Plotting v(Neutral)\n",
      "Plotting a\n",
      "Plotting t\n",
      "Plotting v(Angry)\n",
      "Plotting v(Neutral)\n"
     ]
    },
    {
     "data": {
      "text/plain": "[<hddm.models.hddm_info.HDDM at 0x7f90aa0db828>,\n <hddm.models.hddm_info.HDDM at 0x7f90aa0dbc88>,\n <hddm.models.hddm_info.HDDM at 0x7f90aa32f208>,\n <hddm.models.hddm_info.HDDM at 0x7f90aaeb9c88>]"
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Running group-only accuracy model for model selection\n",
    "\n",
    "models_from_matrix(data2fit=data_acc,filename=input_path+'/matrices/accuracy_model_matrix1.csv',folder=accuracy_coding_path,name='accuracy_testing_all1',sample_no=5000,burn_no=1000,thin_no=2,participant_model=False)\n",
    "\n",
    "# Running winning model with participant nodes and multiple chains\n",
    "\n",
    "run_hddm_multiple(data2fit=data_acc,dependency={'v': 'cond'},folder=accuracy_coding_path,\n",
    "                  name='accuracy_participant1',num_chains=5,sample_no=10000,burn_no=2000,thin_no=5,\n",
    "                  participant_model=True)\n",
    "\n",
    "hddm_save_trace('accuracy_participant1',save=1)\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "outputs": [],
   "source": [
    "# GENERATING A DATAFRAME WITH MEAN V_ANGRY AND MEAN V_NETURAL PER PARTICIPANT\n",
    "\n",
    "traces = pd.read_csv(accuracy_coding_models_path+'accuracy_participant1_trace.csv',index_col=0)\n",
    "\n",
    "ccid = pd.unique(data.subj_idx)\n",
    "\n",
    "# Extract column names which refer to v_angry from big traces dataframe\n",
    "col_names_angry = [col for col in traces.columns if \"v_subj(Angry)\" in col]\n",
    "col_names_neutral = [col for col in traces.columns if \"v_subj(Neutral)\" in col]\n",
    "\n",
    "# Extract the values of v_angry and v_neutral only\n",
    "params_angry = traces.loc[:,col_names_angry]\n",
    "params_neutral = traces.loc[:,col_names_neutral]\n",
    "\n",
    "# Calculate the mean of v_angry and v_neutral for each participant\n",
    "params_angry_mean = params_angry.mean(axis=0)\n",
    "params_neutral_mean = params_neutral.mean(axis=0)\n",
    "\n",
    "# Shorten the names of the row names to include only ccid\n",
    "indexes = []\n",
    "for ix in params_angry_mean.index:\n",
    "    ix = ix[14:]\n",
    "    indexes.append(ix)\n",
    "params_angry_mean.index = indexes\n",
    "\n",
    "indexes = []\n",
    "for ix in params_neutral_mean.index:\n",
    "    ix = ix[16:]\n",
    "    indexes.append(ix)\n",
    "params_neutral_mean.index = indexes\n",
    "\n",
    "d = {'v_angry': params_angry_mean, 'v_neutral': params_neutral_mean}\n",
    "\n",
    "params_v_mean = pd.DataFrame(d)\n",
    "\n",
    "if '610496' in params_v_mean.index:\n",
    "    params_v_mean = params_v_mean.drop('610496')\n",
    "\n",
    "params_v_mean.to_csv(output_path+'params_v_byparticip.csv')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}